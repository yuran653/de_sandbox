version: '3.9'

services:

    airflow-init:
        build:
            context: .
            dockerfile: airflow/Dockerfile
        image: airflow-custom:2.8.4-python3.11
        container_name: airflow-init
        network_mode: host
        env_file:
            - airflow/.env
            - airflow/.airflow.env
        command: >
            bash -c "
            airflow db migrate &&
            airflow users create
            --username $${AIRFLOW_USERNAME}
            --firstname $${AIRFLOW_FIRSTNAME}
            --lastname $${AIRFLOW_LASTNAME}
            --role Admin
            --email $${AIRFLOW_EMAIL}
            --password $${AIRFLOW_PASSWORD} || true
            "
        depends_on:
            metadata-db:
                condition: service_healthy
        volumes:
            - airflow_logs:/opt/airflow/logs

    airflow-scheduler:
        build:
            context: .
            dockerfile: airflow/Dockerfile
        image: airflow-custom:2.8.4-python3.11
        container_name: airflow-scheduler
        cpus: 1.5
        mem_limit: 4g
        network_mode: host
        restart: always
        env_file:
            - airflow/.env
            - airflow/.airflow.env
        command: scheduler
        depends_on:
            airflow-init:
                condition: service_completed_successfully
            metadata-db:
                condition: service_healthy
        volumes:
            - airflow_dags:/opt/airflow/dags
            - airflow_logs:/opt/airflow/logs
            - airflow_plugins:/opt/airflow/plugins
            - airflow_scripts:/opt/airflow/scripts
            - spark_events:/var/spark-events
        healthcheck:
            test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname \"$HOSTNAME\""]
            interval: 30s
            timeout: 10s
            retries: 5

    airflow-webserver:
        build:
            context: .
            dockerfile: airflow/Dockerfile
        image: airflow-custom:2.8.4-python3.11
        container_name: airflow-webserver
        cpus: 0.8
        mem_limit: 1.5g
        network_mode: host
        restart: always
        env_file:
            - airflow/.env
            - airflow/.airflow.env
        command: webserver
        depends_on:
            airflow-scheduler:
                condition: service_healthy
            metadata-db:
                condition: service_healthy
        volumes:
            - airflow_dags:/opt/airflow/dags
            - airflow_logs:/opt/airflow/logs
            - airflow_plugins:/opt/airflow/plugins
            - airflow_scripts:/opt/airflow/scripts
            - spark_events:/var/spark-events
        healthcheck:
            test: ["CMD", "pgrep", "-f", "gunicorn"]
            interval: 30s
            timeout: 10s
            retries: 5

    metadata-db:
        image: postgres:16-alpine3.21
        container_name: metadata-db
        cpus: 0.3
        mem_limit: 300m
        network_mode: host
        restart: always
        env_file:
            - airflow/.env
        environment:
            PGPORT: "5433"
        command: -c port=5433
        volumes:
            - pg_metadata:/var/lib/postgresql/data
        depends_on:
          datalake-db:
            condition: service_healthy
          minio:
            condition: service_healthy
        healthcheck:
            test: ["CMD", "pg_isready", "-U", "airflow", "-p", "5433"]
            interval: 30s
            timeout: 10s
            retries: 5

    datalake-db:
        image: postgres:16-alpine3.21
        container_name: datalake-db
        cpus: 0.8
        mem_limit: 1.4g
        network_mode: host
        restart: always
        env_file:
            - .env
        volumes:
            - pg_datalake:/var/lib/postgresql/data
        healthcheck:
            test: ["CMD", "pg_isready", "-U", "datalake"]
            interval: 30s
            timeout: 10s
            retries: 5

    minio:
        image: minio/minio:RELEASE.2025-09-07T16-13-09Z
        container_name: minio
        cpus: 0.4
        mem_limit: 0.8g
        network_mode: host
        restart: always
        env_file:
            - .env
        command: server /data --console-address ":9001"
        volumes:
            - minio_datalake:/data
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
            interval: 30s
            timeout: 10s
            retries: 5


volumes:

    airflow_dags:
        driver: local
        driver_opts:
            type: none
            o: bind
            device: /airflow/dags

    airflow_logs:
        driver: local
        driver_opts:
            type: none
            o: bind
            device: /airflow/logs

    airflow_plugins:
        driver: local
        driver_opts:
            type: none
            o: bind
            device: /airflow/plugins

    airflow_scripts:
        driver: local
        driver_opts:
            type: none
            o: bind
            device: /airflow/scripts

    spark_events:
        driver: local
        driver_opts:
            type: none
            o: bind
            device: /spark_events

    pg_metadata:
        driver: local
        driver_opts:
            type: none
            o: bind
            device: /pg_metadata

    pg_datalake:
        driver: local
        driver_opts:
            type: none
            o: bind
            device: /pg_datalake

    minio_datalake:
        driver: local
        driver_opts:
            type: none
            o: bind
            device: /minio_datalake
